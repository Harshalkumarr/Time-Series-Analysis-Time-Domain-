{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fetching \n",
    "\n",
    "## Types of Data\n",
    "\n",
    "Data contained in databases, documents, e-mails, and other data files for predictive analysis can be categorized either as structured or unstructured data.\n",
    "\n",
    "1. Structured Data\n",
    "\n",
    "Structured data is well organized, follows a consistent order, is relatively easy to search and query, and can be readily accessed and understood by a person or a computer program.A classic example of structured data is an Excel spreadsheet with labeled columns. Such structured data is consistent; column headers-usually brief, accurate descriptions of the content in each column tell you exactly what kind of content to expect.\n",
    "Structured data is usually stored in well-defined schemas such as databases. It's usually tabular, with columns and rows that clearly define its attributes.\n",
    "\n",
    "2. Unstructured Data\n",
    "\n",
    "Unstructured data, on the other hand, tends to be free-form, non-tabular, dispersed, and not easily retrievable; such data requires deliberate intervention to make sense of it. Miscellaneous e-mails, documents, web pages, and files (whether text, audio, and/or video) in scattered locations are examples of unstructured data.\n",
    "\n",
    "It's hard to categorize the content of unstructured data. It tends to be mostly text, it's usually created in a free-form styles, and finding any attributes you can use to describe or group it is no small task.\n",
    "\n",
    "The content of unstructured data is hard to work with or make sense of programmatically. Computer programs cannot analyze or generate reports on such data, simply because it lacks structure, has no underlying dominant characteristic, and individual items of data have no common ground.\n",
    "\n",
    "Unstructured data requires more work to make it useful, so it gets more attention — thus tends to consume more time.\n",
    "\n",
    "The resultant newly organized data from those necessary preprocessing steps can then be used in a predictive analytics model. The wholesale transformation of unstructured data however, may have to wait until you have your predictive analytics model up and running.\n",
    "\n",
    "Data mining and text analytics are two approaches to structuring text documents, linking their contents, grouping and summarizing their data, and uncovering patterns in that data. Both disciplines provide a rich framework of algorithms and techniques to mine the text scattered across a sea of documents.\n",
    "\n",
    "### Structured Data Fetching\n",
    "\n",
    "   Structured data (arranged in well defined rows and columns) is spread across different relational databases in different tables.To prepare our dataset for analysis, we have to fetch it from different tables of respective databases.\n",
    "    SQL (Structured  enables us to fetch and manipulate data) using following categories:\n",
    "    \n",
    "1. DDL (Data Definition Language):\n",
    "Defines database-create,drop,alter,truncate,comment,rename etc\n",
    "2. DML (Data Manipulation Language):\n",
    "Manipulates data in database-select,insert,update,delete.\n",
    "3. DCL (Data Control Language):\n",
    "Rights and permission to access database-Grant,revoke\n",
    "4. TCL (Transaction Control Language):\n",
    "Transactions within database-commit,rollback,savepoint,set transactions\n",
    "5. Clauses:\n",
    "First hand Sorting and Filtering data-having,group by,order by,where,top\n",
    "\n",
    "6.Primary and Foreign key - We use Primary Key to uniquely identify each Record in a table. The Primary Key is consist of single or multiple columns. Whereas Foreign Key is a set of multiple columns in a table that indicates the primary key in the alternate table.\n",
    "\n",
    "7.Null Value - We use the term Null Value in SQL to express the missing value. It is a mark in SQL to represent that the value of data does not exist in the database. There is a difference between a Null Value and a zero value or a table that has spaces.\n",
    "\n",
    "8.Subquery - Subquery also called as Nested Query or Insert Query. This means a query in another query, generally insert in WHERE Clause. There are 4 types of Subquery that are with a SELECT statement, INSERT statement, the UPDATE statement, DELETE statement. A Subquery can return a set of records to its primary query.\n",
    "\n",
    "9.Indexes - With the help of SQL indexes we can find out the data easily and quickly, no need to search every row in a database table. That is SQL index quickly load the data. We use this to speed up searches. Index classified in 2 types - Clustered and Non-Clustered Indexes.\n",
    "\n",
    "10.Joins - SQL Joins combines rows from two or more tables. SQL Joins are of 4 types - INNER, LEFT, RIGHT, FULL join.\n",
    "\n",
    "\n",
    "\n",
    "For More Information,visit:https://github.com/Harshalkumarr/Data-Fetching-in-Python-from-MySQL-server/blob/master/PlayWithDatabase.ipynb \n",
    "\n",
    "# Data Transformation & Cleaning\n",
    "\n",
    "After fetching the data in required form,we have to read dataset by using functions of our language (tools-R,SAS,python etc.).Here we are using Python as an analytical tool for our assignments.\n",
    "\n",
    "In python,'pandas' library has two data structure viz. DataFrame and Series to represent data.Following operations can be performed on these data structures:\n",
    "\n",
    "1. Indexing and object selection.\n",
    "2. Multi-indexing (enables us to store and manipulate data with an arbitrary number of dimensions in lower dimensional data structures like Series (1d) and DataFrame (2d)).\n",
    "3. Combining series,dataframe,panel objects (join,merge,concat).\n",
    "4. pivoting\n",
    "\n",
    "By using above operations,we have to clean the data and transform the data if necessary.\n",
    "\n",
    "   *Data transformation and cleaning along with data visualization is the key to success for any Data Scientist.Many full stack Data Scientists invests their 60-70% of time in these two steps*\n",
    "\n",
    "\n",
    "# Data Visualization (Exploratory Data Analysis)/Data Communication\n",
    "Choice of visualization technique depends on type of variable.There are two mainly two types of variables viz. Categorical(Qualitative) and Numerical(Quantitative).\n",
    "\n",
    "1. Categorical variable:\n",
    "Represents characteristics.e.g.Education of person-Graduate or Non-graduate.\n",
    "Categorical data is further classified into two types namely Nominal and Ordinal.\n",
    "        a) Nominal variable\n",
    "Takes discrete values but they don't have quantitative value e.g.Yes and No can be shown by 1 and 0 respectively. \n",
    "        b) Ordinal variable\n",
    "It takes ordered discrete values and they also don't have quantitative value.e.g.Numbers of Dependents in family (1,2,3...)\n",
    "\n",
    "2. Numerical variable\n",
    "It takes values that can be measured and counted.e.g.Loan Amount,Applicant Income,Coapplicant Income.Numerical variable is further classified into two types viz.Discrete and Continuous\n",
    "        a) Discrete variable\n",
    "Discrete type of variable takes either finitely or infinitely countable values.\n",
    "        b) Continuous variable\n",
    "Continuous takes values that can be measured on scale but can't be countable.e.g.height,weight.Continuous variable is further classified into Interval variable and Ratio variable.\n",
    "            b(i) Interval variable\n",
    "It takes values on scale(units on scale has same difference).It don't have a true zero.\n",
    "            b(ii) Ratio variable\n",
    "Ratio values are the same as interval values, with the difference that they do have an absolute zero.\n",
    "\n",
    "Following table shows that the metrics by which we can summarize our data/variables:\n",
    "\n",
    "| OK to summarize variable|\tNominal\t| Ordinal\t|Interval\t|Ratio |\n",
    "|--------------|------------|-----------|-----------|------|\n",
    "|frequency distribution,proportion|Yes\t |   Yes\t|       Yes\t|     Yes|\n",
    "|median and percentiles,IQR,Mode|\tNo\t|Yes\t|Yes\t|Yes|\n",
    "|add or subtract\t|No\t|No\t|Yes|\tYes|\n",
    "|mean, standard deviation, standard error of the mean,median,mode,percentiles,IQR,range|\tNo\t|No\t|Yes\t|Yes|\n",
    "|ratio,or coefficient of variation|\tNo|\tNo|\tNo|\tYes|\n",
    " \t \t \t \n",
    "##  Visualization of Numerical (Quantitative) Variables ( Univariate Analysis)\n",
    "### A. Time Series Data\n",
    "    \n",
    "   Time series data variable takes values evenly spaced in time and measured successively.Order of observations matters in time series data since there is dependency and changing the order can change the meaning of data. Goals of time series analysis:\n",
    "    \n",
    "   a) Identifying the nature of the phenomenon represented by the sequence of observations.\n",
    "   b) To explain how the past affects the future or how two time series can “interact”.\n",
    "   c) Forecasting (predicting future values of the time series variable)\n",
    "   d) Intervention analysis: how does a single event change the time series?\n",
    "   e) Quality control: deviations of a specified size indicate a problem\n",
    "  \n",
    "  Time series are analyzed in order to understand the underlying structure and function that produce the observations.Understanding the mechanisms of a time series allows a mathematical model to be developed that explains the data in such a way that prediction, monitoring, or control can occur.  \n",
    "There are two main approaches used to analyze time series: \n",
    "\n",
    "(1) Time Domain Analysis\n",
    "\n",
    "Before analysis,we need to understand one important feature  of time series-\"stationarity\".\n",
    "\n",
    "Stationarity of time series:\n",
    "\n",
    "In time series analysis, one of the fundamental requirements is to test the stationarity of series (i.e. unit root). Literary, it involves two things:\n",
    "\n",
    "A Stationary Series is a Variable with constant Mean across time\n",
    "\n",
    "A Stationary Series is a Variable with constant Variance across time\n",
    "\n",
    "There are number of methods available to check the stationarity of time series. These include auto correlation function, Dickey Fuller test, Augmented Dickey Fuller test, etc.\n",
    "\n",
    "Different types of stochastic processes that are useful in modeling time series:\n",
    "\n",
    "a) Purely random process\n",
    "\n",
    "b) Random walk process\n",
    "\n",
    "c) Moving average (MA) process\n",
    "\n",
    "d) Autoregressive (AR) process\n",
    "\n",
    "e) Autoregressive moving average (ARMA) process\n",
    "\n",
    "f) Autoregressive integrated moving average (ARIMA) process\n",
    "\n",
    "Some of the important above processes have been discussed below in brief:\n",
    "\n",
    "**Moving average (ma) process**\n",
    "\n",
    "A moving average (MA) process is essentially created by lagging the residual term of the time series. For example a time series x with an autoregressive (AR) process of length 1 and a moving average process of length 2 can be expressed as x(t) = x(t-1) + e(t) + e(t-1) + e(t-2). In general, a time series can be represented by an ARMA (p, q) process, where p denotes the length of the autoregressive process, while q denotes the length of the moving average process.\n",
    "\n",
    "**Autoregressive (ar) process**\n",
    "\n",
    "Many observed time series exhibit serial autocorrelation; that is, linear association between lagged observations. This suggests past observations might predict current observations. The autoregressive (AR) process models the conditional mean of Y$t$ as a function of past observations, i.e. Y$t-1$, Y$t-2$, .... Y$t-p$. An AR process that depends on p past observations is called an AR model of degree p, denoted by AR (p). The AR model can be represented as follows:\n",
    "\n",
    "$y$t$=c+By$t-1$+.....+B$p$y$t-p$+E$t\n",
    "\n",
    "where Et is an uncorrelated innovation process with mean zero.\n",
    "\n",
    "**Autoregressive moving average (arma) process**\n",
    "\n",
    "For some observed time series, a very high-order AR or MA model is needed to model the underlying process well. In this case, a combined autoregressive moving average (ARMA) model can sometimes be a more parsimonious choice. An ARMA model expresses the conditional mean of yt as a function of both past observations y$(t-1)$,....y$(t-p)$  and past innovations E$(t-1)$....E$(t-q)$  The number of past observations that yt depends on, p, is the AR degree. The number of past innovations that yt depends on, q, is the MA degree. In general, these models are denoted by ARMA (p,q).\n",
    "\n",
    "\n",
    "The usual ARMA model is like this\n",
    "\n",
    "$y$t$=c+By$t-1$+.....+B$p$y$t-p$+E$t+E$t$+T$1$E$t-1$+....+T$q$E$t-q$\n",
    "\n",
    "where E$t$ is an uncorrelated innovation process with mean zero.\n",
    "\n",
    "**Autoregressive integrated moving average (arima) process**\n",
    "\n",
    "The autoregressive integrated moving average (ARIMA) process generates nonstationary series that are integrated of order D, denoted I (D). A nonstationary I (D) process is one that can be made stationary by taking D differences. Such processes are often called difference-stationary or unit root processes.\n",
    "\n",
    "A series that you can model as a stationary ARMA (p,q) process after being differenced D times is denoted by ARIMA (p,D,q). \n",
    "\n",
    "Goodness of fit of time series models\n",
    "\n",
    "When an AR, MA, or ARMA model has been fitted to a given time series, it is advisable to check that the model does really give an adequate description of the data\n",
    "\n",
    "There are two criteria often used that reflect the closeness of fit and the number of parameters estimated.\n",
    "\n",
    "One is the Akaike information criterion (AIC), and the other is the Schwartz Bayesian criterion (SBC)\n",
    "\n",
    "The latter is also called the Bayesian information criterion (BIC).\n",
    "\n",
    "**Box jenkins approach**\n",
    "\n",
    "The Box-Jenkins approach is one of the most widely used methodologies for the analysis of time-series data.\n",
    "\n",
    "It is popular because of its generality; it can handle any series, stationary or not, with or without seasonal elements, and it has well-documented computer programs.\n",
    "\n",
    "Although Box and Jenkins have been neither the originators nor the most important contributors in the field of ARMA models.\n",
    "\n",
    "They have popularized these models and made them readily accessible to everyone, so much that ARMA models are sometimes referred to as Box-Jenkins models.\n",
    "\n",
    "**The basic steps in the box-jenkins methodology**\n",
    "\n",
    "i) Differencing the series so as to achieve stationarity\n",
    "\n",
    "ii) Identification of a tentative model\n",
    "\n",
    "iii) Estimation of the model\n",
    "\n",
    "iv) Diagnostic checking\n",
    "\n",
    "v) Using the model for forecasting and control\n",
    "\n",
    "The above points have been discussed in brief below:\n",
    "\n",
    "Step (i):\n",
    "\n",
    "    a) Differencing to achieve stationarity: How do we conclude whether a time series is stationary or not?\n",
    "\n",
    "\n",
    "    b) We can do this by studying the graph of the autocorrelation of the series.\n",
    "\n",
    "\n",
    "    c) The correlogram/autocorrelation of a stationary series drops off as k, the number of lags, becomes large, but this is not usually the case for a nonstationary series.\n",
    "\n",
    "\n",
    "    d) Thus the common procedure is to plot the correlogram of the given series yt and successive differences yt-1   and so on, and look at the correlograms at each stage.\n",
    "\n",
    "\n",
    "    e) We keep differencing until the correlogram dampens.\n",
    "\n",
    "Step (ii):\n",
    "\n",
    "Once we have used the differencing procedure to get a stationary time series, we examine the correlogram to decide on the appropriate orders of the AR and MA components.\n",
    "\n",
    "    a) The correlogram of a MA process is zero after a point.\n",
    "\n",
    "\n",
    "    b) That of an ARprocess declines geometrically. The correlograms of ARMA processes show different patterns.\n",
    "\n",
    "\n",
    "    c) Based on these, one arrives at a tentative ARMA model.\n",
    "\n",
    "\n",
    "    d) This step involves more of a judgmental procedure than the use of any clear-cut rules.\n",
    "\n",
    "Step (iii):\n",
    "\n",
    "The next step is the estimation of the tentative ARMA model identified in step 2. We have discussed in the preceding section the estimation of ARMA models.\n",
    "\n",
    "Step (iv):\n",
    "\n",
    "The next step is diagnostic checking to check the adequacy of the tentative model. We discussed in the preceding section the Q and Q* statistics commonly used in diagnostic checking.\n",
    "\n",
    "Step (v):\n",
    "\n",
    "The final step is forecasting. It is the process of making predictions about the economy. Forecasts can be carried out at a high level of aggregation - for example for GDP, inflation, unemployment or the fiscal deficit - or at a more disaggregated level, for specific sectors of the economy or even specific firms.\n",
    "\n",
    "**Unit Root Test**\n",
    "\n",
    "This module attempts to know the stationarity of variables in the modelling system. This is very essential condition for various top classes modelling like VAR, VECM, etc. In this section, we highlight the followings for the simultaneous equation modeling:\n",
    "\n",
    "1. What is unit root\n",
    "\n",
    "2. Methods of Unit Root Test\n",
    "\n",
    "3. Estimation and testing for unit root\n",
    "\n",
    "**What is unit root test**\n",
    "\n",
    "A unit root is a feature of processes that evolve through time that can cause problems in statistical inference if it is not adequately dealt with.\n",
    "\n",
    "It is very common for time series data to demonstrate signs of non-stationarity; particularly both mean and variance of variables trend upwards over time. In any case, test of non-stationarity are carried out as a preliminary step to explore the possibility of a significant long run relationship between the variables concerned, i. e. cointegration tests. The test for nonstationarity is to know the order of integration, where the time series variables are stationary. A linear stochastic process has a unit root if 1 is a root of the process's characteristic equation. Such a process is non-stationary. If the other roots of the characteristic equation lie inside the unit circle- that is, have a modulus less than one- then the first difference of the process will be stationary.\n",
    "\n",
    "Shocks to a unit root process have permanent effects which do not decay as they would if the process were stationary\n",
    "\n",
    "As noted above, a unit root process has a variance that depends on t, and diverges to infinity\n",
    "\n",
    "If it is known that a series has a unit root, the series can be differenced to render it stationary. For example, if a series Yt is I (1), the series ΔYt=Yt-Yt-1 is I(0) (stationary). It is hence called a difference stationary series.\n",
    "\n",
    "**Methods of unit root test**\n",
    "\n",
    "There are several methods through which we can check the stationary (or unit root) in the time series setting. It starts with simple autoregressive scheme, which is as follows:\n",
    "\n",
    "yt=A+Q yt-1+Et\n",
    "\n",
    "Where, Q is autoregressive (AR) coefficient. If Q=1, we can say there is unit root and it is nonstationary. That means it indicates that the mean and variance are non-constant and violates the normal requirement of time series modelling.\n",
    "\n",
    "The methods to check the stationarity are as follows:\n",
    "\n",
    "Dickey fuller\n",
    "\n",
    "Augmented dickey fuller\n",
    "\n",
    "Phillips and perron\n",
    "\n",
    "Kwiatkowski- phillips-schmidt-shin test\n",
    "\n",
    "Elliott-rothenberg stock test\n",
    "\n",
    "Among them, the three most important and easiest are Dickey Fuller (DF), Augmented Dickey Fuller (ADF) and Phillips and Perron (PP). The detail discussions of these two tests are as follows:\n",
    "\n",
    "Dickey fuller test:\n",
    "\n",
    "The test follows at three levels:\n",
    "\n",
    "1. with constant and trend\n",
    "\n",
    "2. with Constant only\n",
    "\n",
    "3. with Constant and trend\n",
    "\n",
    "We use three different models for the estimation process for the above three cases. These are as follows:\n",
    ".........\n",
    "\n",
    "(2) Frequency Domain Analysis\n",
    "\n",
    "####  Time Domain Analysis\n",
    "\n",
    "**a) ARIMA (Autoregressive Integrated Moving Average) Model:**\n",
    "\n",
    "   Analysis in the time domain is most often used **for stochastic (random or non-deterministic event/process) observations**.Time series may reflect the stochastic nature of most measurements over time.Thus,data may be skewed, with mean and variation not constant, non-normally distributed, and not randomly sampled or independent.Another non-normal aspect of time series observations is that they are often not evenly spaced in time due to instrument failure, or simply due to variation in the number of days in a month\n",
    "   \n",
    "   One common technique to analyze this stochastic nature of time series is the Box-Jenkins ARIMA method, which can be used for univariate (a single data set) or multivariate (comparing two or more data sets) analyses.  The ARIMA technique uses moving averages, detrending, and regression methods to detect and remove autocorrelation in the data.  \n",
    "   \n",
    "   In time series analysis, it is **assumed** that the data consist of a **Systematic pattern (usually a set of identifiable components)** and **Random noise (error)** which usually makes the pattern difficult to identify.Most time series analysis techniques involve some form of filtering out noise in order to make the pattern more salient\n",
    "   \n",
    "   In order to analyze it,we try to decompose time series into these systematic patterns and random noise that prevent us to identify such patterns.This process is called as Time Series Decomposition.\n",
    "   We try to decompose Time Series in such a way that it will show following patterns and try to model each of them independently.\n",
    "\n",
    "**1. Trend Analysis**:\n",
    "  *Trend is a pattern that shows us overall direction of series e.g. upward or downward or any other.*\n",
    "  \n",
    "  There are no proven \"automatic\" techniques to identify trend components in the time series data; however, as long as the trend is monotonous (consistently increasing or decreasing) that part of data analysis is typically not very difficult. If the time series data contain considerable error, then the first step in the process of trend identification is **smoothing**\n",
    "\n",
    "   **Smoothing:** Smoothing always involves some form of *local averaging of data* such that the nonsystematic components of individual observations cancel each other out. The most common technique is **Moving Average Smoothing** which replaces each element of the series by either the simple or weighted average of n surrounding elements, where n is the width of the smoothing \"window\" (see Box & Jenkins, 1976; Velleman & Hoaglin, 1981).\n",
    "   \n",
    "   **Medians** can be used instead of means. The main advantage of median as compared to moving average smoothing is that its results are less biased by outliers (within the smoothing window). Thus, if there are outliers in the data (e.g., due to measurement errors), median smoothing typically produces smoother or at least more \"reliable\" curves than moving average based on the same window width. The main disadvantage of median smoothing is that in the absence of clear outliers it may produce more \"jagged\" curves than moving average and it does not allow for weighting.\n",
    "\n",
    "   In the relatively less common cases (in time series data), when the **measurement error is very large, the distance weighted least squares smoothing or negative exponentially weighted smoothing techniques** can be used. All those methods will filter out the noise and convert the data into a smooth curve that is relatively unbiased by outliers (see the respective sections on each of those methods for more details). Series with relatively few and systematically distributed points can be smoothed with bicubic splines.\n",
    "\n",
    "   Fitting a function: Many monotonous time series data can be adequately approximated by a linear function; if there is a clear monotonous nonlinear component, the data first need to be transformed to remove the nonlinearity. Usually a logarithmic, exponential, or (less often) polynomial function can be used.\n",
    "\n",
    "\n",
    "\n",
    "**2. Analysis of Seasonality:**\n",
    "*Seasonality is a trend that repeats itself systematically over time*\n",
    "\n",
    "   Seasonal dependency (seasonality) is another general component of the time series pattern.It is formally defined as correlational dependency of order k between each i'th element of the series and the (i-k)'th element (Kendall, 1976) and measured by autocorrelation (i.e., a correlation between the two terms); k is usually called the lag. If the measurement error is not too large, seasonality can be visually identified in the series as a pattern that repeats every k elements.\n",
    "   \n",
    "   We can observe seasonality: \n",
    "   \n",
    "   1) using function **lag_plot()** in **pandas.plotting** module.\n",
    "   \n",
    "Import it as:\n",
    "   \n",
    "   **from pandas.plotting import lag_plot**\n",
    "   \n",
    "   2) Autocorrelation plot:\n",
    "   \n",
    "Import it as:\n",
    "\n",
    "**from pandas.plotting import autocorrelation_plot**\n",
    "   \n",
    "\n",
    "**2.1 Lag plot**:\n",
    "\n",
    "  It is a type of scatter plot.Lag plots are used to check if a data set or time series is random. Random data should not exhibit any structure in the lag plot. Non-random structure implies that the underlying data are not random. The lag argument may be passed, and when lag=1 the plot is essentially data[:-1] vs. data[1:].\n",
    "  \n",
    "Selection of appropriate lag value:\n",
    "\n",
    "  Box and Jenkins wrote, “..to obtain a useful estimate of the autocorrelation function, we would need at least 50 observations and the estimated autocorrelations would be calculated for k = 0, 1, …, k, where k was not larger than N/4”.  For my data set of 78 observations, I specified 19 autocorrelation lags (78/4 = 19.5).\n",
    "\n",
    "Lag plot allows an analyst to check:\n",
    "\n",
    "1. Seasonality\n",
    "2. Model Suitability\n",
    "3. Outliers\n",
    "4. Randomness\n",
    "5. Serial Correlation/Dependency\n",
    "\n",
    "1.Seasonality:\n",
    "\n",
    "2.Model Suitability:\n",
    "\n",
    "If data shows some trend or pattern,Analyst can conclude that data is came from some process which can be modeled by mathematical function (mathematical model).For example,if data shows linear trend,plot suggests that an *Autoregressive Model* will define the data properly.\n",
    "\n",
    "3.Outliers:\n",
    "\n",
    "A value that \"lies outside\" (is much smaller or larger than) most of the other values in a set of data.These observations looks distinct,generally away from the observed pattern (if seasonality occurred in data)  \n",
    "\n",
    "4.Randomness (no seasonality)\n",
    "No well-defined pattern in graph.\n",
    "\n",
    "5.Serial Correlation/Autocorrelation:\n",
    "Crowding of data along diagonal (either +ve or -ve ) i.e if there is linear pattern.\n",
    "\n",
    "**2.2 Autocorrelation/Serial Correlation plot**:\n",
    "\n",
    "   Autocorrelation plots are often used for checking randomness in time series. This is done by computing autocorrelations for data values at varying time lags. If time series is random, such autocorrelations should be near zero for any and all time-lag separations. If time series is non-random then one or more of the autocorrelations will be significantly non-zero. The horizontal lines displayed in the plot correspond to 95% and 99% confidence bands. The dashed line is 99% confidence band. See the Wikipedia entry for more about autocorrelation plots.\n",
    "\n",
    "\n",
    "**Interpreting Lag plot and Autocorrelation plot:**\n",
    "\n",
    "Example 1) Data: Monthly precipitation data (Number of observations=76) collected from January 1999 through April 2004.Let's plot the data,\n",
    "\n",
    "lag value=Number of Observation/4 (Box and Jenkin)=76/4=19\n",
    "\n",
    "\n",
    "<img src=\"MPI.png\">\n",
    "\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "There is no consistent trend (upward or downward) over entire time span.\n",
    "\n",
    "\n",
    "\n",
    "**Interpreting Autocorrelation plot with all possible lags**:\n",
    "\n",
    "<img src=\"autocorrelation_precipitation_plot.png\">\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "Above autocorrelation plot shows strength of autocorrelation with all possible lags.There is deviations in plot at lag values 6 or 7 probably and at lag value of 12.So we can interpret that there are two seasonal (rainy season and dry season) patterns spaced about 6 months apart.   \n",
    "\n",
    "**Interpreting Lag Plot**:\n",
    "\n",
    "<img src=\"Lag.png\">\n",
    "\n",
    "\n",
    "#### Frequency Domain Analysis\n",
    "Analysis in the frequency domain is often used for periodic and cyclical observations. Common techniques are spectral analysis, harmonic analysis, and periodogram analysis.  A specialized technique is Fast Fourier Transform (FFT).  Mathematically, frequency domain techniques use fewer computations than time domain techniques, thus for complex data, analysis in the frequency domain is most common.  However, frequency analysis is more difficult to understand, so time domain analysis is generally used outside of the sciences.\n",
    "\n",
    "For frequency domain analysis\n",
    "visit:https://github.com/Harshalkumarr/Frequency-Domain-Analysis-of-Speech-Signal\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
